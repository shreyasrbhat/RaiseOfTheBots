{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import codecs\n",
    "import csv\n",
    "import collections\n",
    "import unicodedata\n",
    "import itertools\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"cornell_movie_dialogs_corpus\"\n",
    "data = os.path.join('data', data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print first few lines of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few lines in movie_lines.txt\n",
      "L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\n",
      "\n",
      "L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\n",
      "\n",
      "L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.\n",
      "\n",
      "L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?\n",
      "\n",
      "L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\n",
      "\n",
      "First few lines in movie converstations file\n",
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']\n",
      "\n",
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']\n",
      "\n",
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']\n",
      "\n",
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L204', 'L205', 'L206']\n",
      "\n",
      "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L207', 'L208']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_lines(dataset):\n",
    "    with open(dataset, encoding='iso-8859-1') as file:\n",
    "        print(\"\\n\".join(file.readlines()[:5]))\n",
    "print(\"First few lines in movie_lines.txt\")\n",
    "print_lines(os.path.join(data,\"movie_lines.txt\"))\n",
    "print(\"First few lines in movie converstations file\")\n",
    "print_lines(os.path.join(data, \"movie_conversations.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### untils functions to process coversations in the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lines_to_dict(file_name, fields):\n",
    "    \"\"\"\n",
    "    parse attributes of lines into dict\n",
    "    \"\"\"\n",
    "    lines = {}\n",
    "    with open(file_name, 'r', encoding='iso-8859-1') as file:\n",
    "        for line in file:\n",
    "            lineObj = {}\n",
    "            attrs = line.split(' +++$+++ ')\n",
    "            for j, field in enumerate(fields):\n",
    "                lineObj[field] = attrs[j]\n",
    "            lines[lineObj[\"lineID\"]] = lineObj\n",
    "    return lines\n",
    "                \n",
    "\n",
    "def load_conversations(file_name, fields, lines):\n",
    "    \"\"\"\n",
    "    group each conversation into dict\n",
    "    \"\"\"\n",
    "    conversations = []\n",
    "    with open(file_name, 'r', encoding='iso-8859-1') as file:\n",
    "        for line in file:\n",
    "            convObj={}\n",
    "            attrs = line.split(' +++$+++ ')\n",
    "            for i, field in enumerate(fields):\n",
    "                convObj[field] = attrs[i]\n",
    "            regex = re.compile('L[0-9]+')\n",
    "            lineIds = regex.findall(convObj[\"utteranceIDs\"])\n",
    "            convObj['lines'] = []\n",
    "            for lineId in lineIds:\n",
    "                convObj['lines'].append(lines[lineId])\n",
    "            conversations.append(convObj)\n",
    "    return conversations\n",
    "\n",
    "\n",
    "def load_conv_pairs(conversations):\n",
    "    \"\"\"\n",
    "    get each pairs of utterence and reply from conversations\n",
    "    \"\"\"\n",
    "    conv_pairs = []\n",
    "    for convObj in conversations:\n",
    "        for i in range(len(convObj['lines'])-1):\n",
    "            utter = convObj['lines'][i][\"text\"].strip()\n",
    "            reply = convObj['lines'][i+1][\"text\"].strip()\n",
    "            if utter and reply:\n",
    "                conv_pairs.append([utter, reply])\n",
    "    return conv_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing corpus...\n",
      "\n",
      " loading conversations\n"
     ]
    }
   ],
   "source": [
    "corpus = os.path.join('data', 'formated_corpus.txt')\n",
    "\n",
    "MOVIE_LINES_FIELDS = [\"lineID\", \"characterID\", \"movieID\", \"character\", \"text\"]\n",
    "MOVIE_CONVERSATIONS_FIELDS = [\"character1ID\", \"character2ID\", \"movieID\", \"utteranceIDs\"]\n",
    "\n",
    "print(\"\\nProcessing corpus...\")\n",
    "lines = load_lines_to_dict(os.path.join(data,\"movie_lines.txt\"), MOVIE_LINES_FIELDS)\n",
    "print('\\n loading conversations')\n",
    "conversations = load_conversations(os.path.join(data, \"movie_conversations.txt\"), MOVIE_CONVERSATIONS_FIELDS,\n",
    "                                   lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing conversation pair to csv file\n"
     ]
    }
   ],
   "source": [
    "delimiter = '\\t'\n",
    "delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n",
    "print('writing conversation pair to csv file')\n",
    "with open(corpus, 'w', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file, delimiter=delimiter, lineterminator='\\n')\n",
    "    for pair in load_conv_pairs(conversations):\n",
    "        writer.writerow(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a0ed419ef6db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'corpus' is not defined"
     ]
    }
   ],
   "source": [
    "print_lines(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0  # Used for padding short sentences\n",
    "SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 2  # End-of-sentence token\n",
    "MAX_length = 10\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self):\n",
    "        self.voc = []\n",
    "        self.word_to_index = {'PAD': PAD_token, 'SOS': SOS_token, 'EOS': EOS_token }\n",
    "        self.index_to_word = {}\n",
    "        self.trim = True\n",
    "        self.min_count = 3\n",
    "        self.max_length = 10\n",
    "        self.vocab_pairs = []\n",
    "    \n",
    "\n",
    "    def unicodeToAscii(self, s):\n",
    "        return ''.join(\n",
    "            c for c in unicodedata.normalize('NFD', s)\n",
    "            if unicodedata.category(c) != 'Mn'\n",
    "        )\n",
    "    \n",
    "    def normalizeSentence(self, sentence):\n",
    "        s = self.unicodeToAscii(sentence.lower().strip())\n",
    "        s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "        s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "        s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "        return s\n",
    "    \n",
    "    def update_vocab(self, sentence):\n",
    "        sentence = self.normalizeSentence(sentence)\n",
    "        self.voc.extend(sentence.split())\n",
    "    \n",
    "    def generate_vocab(self):\n",
    "        reserve_words_count = len(self.word_to_index)\n",
    "        word_count = dict(collections.Counter(self.voc).most_common())\n",
    "        if self.trim:\n",
    "            word_count = dict(filter(lambda x: x[1] >= self.min_count, word_count.items()))\n",
    "        self.word_to_index.update({word:i+reserve_words_count for i, word in enumerate(word_count.keys())})\n",
    "        self.index_to_word = {i:word for word, i in self.word_to_index.items()}\n",
    "    \n",
    "    def filter_pairs(self, pair):\n",
    "        pair[0] = self.normalizeSentence(pair[0])\n",
    "        pair[1] = self.normalizeSentence(pair[1])\n",
    "        eliminate = False\n",
    "        if (len(pair[0].split()) >= self.max_length) or (len(pair[1].split()) >= self.max_length):\n",
    "            return\n",
    "        for sentence in pair:\n",
    "            if len(set(sentence.split()) - self.word_to_index.keys()) > 0:\n",
    "                eliminate = True\n",
    "        if not eliminate:\n",
    "            self.vocab_pairs.append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pickle file available\n",
    "# voc = Vocab()\n",
    "# with open(corpus,'r',encoding='utf-8') as file:\n",
    "#     for line in file:\n",
    "#         pair = line.split('\\t')\n",
    "#         voc.update_vocab(pair[0])\n",
    "#         voc.update_vocab(pair[1])\n",
    "# voc.generate_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(corpus, 'r', encoding='utf-8') as file:\n",
    "#     for line in file:\n",
    "#         pair = line.split('\\t')\n",
    "#         voc.filter_pairs(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('voc.pickle', 'wb') as file:\n",
    "#     pickle.dump(voc, file)\n",
    "with open('voc.pickle', 'rb') as file:\n",
    "    voc = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['there .', 'where ?']\n",
      "['you have my word . as a gentleman', 'you re sweet .']\n",
      "['hi .', 'looks like things worked out tonight huh ?']\n",
      "['you know chastity ?', 'i believe we share an art instructor']\n",
      "['have fun tonight ?', 'tons']\n",
      "['well no . . .', 'then that s all you had to say .']\n",
      "['then that s all you had to say .', 'but']\n",
      "['but', 'you always been this selfish ?']\n",
      "['do you listen to this crap ?', 'what crap ?']\n",
      "['what good stuff ?', 'the real you .']\n"
     ]
    }
   ],
   "source": [
    "for pairs in voc.vocab_pairs[:10]:\n",
    "    print(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTransform:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def index_from_sentence(self, word_to_index, sentence):\n",
    "        return [word_to_index[word] for word in sentence.split()] + [EOS_token]\n",
    "    \n",
    "    def zero_padding(self, encoded_list, fill_value=PAD_token):\n",
    "        return list(itertools.zip_longest(*encoded_list, fillvalue=PAD_token))\n",
    "    \n",
    "    def mask_matrix(self, l, value=PAD_token):\n",
    "        m = []\n",
    "        for i, seq in enumerate(l):\n",
    "            m.append([])\n",
    "            for index in seq:\n",
    "                if index==value:\n",
    "                    m[i].append(0)\n",
    "                else:\n",
    "                    m[i].append(1)\n",
    "        return m\n",
    "     \n",
    "    def inputVar(self, voc, l):\n",
    "        index_batch = [self.index_from_sentence(voc, sentence) for sentence in l]\n",
    "        lengths = torch.tensor([len(indexes) for indexes in index_batch])\n",
    "        zero_pad = self.zero_padding(index_batch)\n",
    "        padVar = torch.LongTensor(zero_pad)\n",
    "        return padVar, lengths\n",
    "    \n",
    "    def outputVar(self, voc, l):\n",
    "        index_batch = [self.index_from_sentence(voc, sentence) for sentence in l]\n",
    "        max_target_length = max([len(indexes) for indexes in index_batch])\n",
    "        zero_pad = self.zero_padding(index_batch)\n",
    "        mask = self.mask_matrix(zero_pad)\n",
    "        mask = torch.BoolTensor(mask)\n",
    "        padVar = torch.LongTensor(zero_pad)\n",
    "        return padVar, mask, max_target_length\n",
    "    \n",
    "    def batch2Train(self, voc, pair_batch):\n",
    "        pair_batch.sort(key = lambda x:len(x[0].split()), reverse = True)\n",
    "        input_batch, output_batch = [], []\n",
    "        for pair in pair_batch:\n",
    "            input_batch.append(pair[0])\n",
    "            output_batch.append(pair[1])\n",
    "        inp, lengths = self.inputVar(voc, input_batch)\n",
    "        out, out_mask, target_length = self.outputVar(voc, output_batch)\n",
    "        return inp, lengths, out, out_mask, target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "textTransform = TextTransform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variable: tensor([[    4,     4,    30,    47,   412],\n",
      "        [   30,    28,   590,     3,     3],\n",
      "        [   32, 10972,   282,     2,     2],\n",
      "        [  370,     6,     6,     0,     0],\n",
      "        [    3,     2,     2,     0,     0],\n",
      "        [   77,     0,     0,     0,     0],\n",
      "        [    9,     0,     0,     0,     0],\n",
      "        [ 1604,     0,     0,     0,     0],\n",
      "        [    2,     0,     0,     0,     0]])\n",
      "lengths: tensor([9, 5, 5, 3, 3])\n",
      "target_variable: tensor([[    4,     5,  4241,    86,   381],\n",
      "        [   29,   172,     2,     6,    45],\n",
      "        [  753,    21,     0,     2,   161],\n",
      "        [    3,  1094,     0,     0,   660],\n",
      "        [    2,    83,     0,     0,    51],\n",
      "        [    0,   987,     0,     0,   282],\n",
      "        [    0, 13728,     0,     0,   211],\n",
      "        [    0,     2,     0,     0,     6],\n",
      "        [    0,     0,     0,     0,     2]])\n",
      "mask: tensor([[ True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True],\n",
      "        [ True,  True, False,  True,  True],\n",
      "        [ True,  True, False, False,  True],\n",
      "        [ True,  True, False, False,  True],\n",
      "        [False,  True, False, False,  True],\n",
      "        [False,  True, False, False,  True],\n",
      "        [False,  True, False, False,  True],\n",
      "        [False, False, False, False,  True]])\n",
      "max_target_len: 9\n"
     ]
    }
   ],
   "source": [
    "small_batch_size = 5\n",
    "batches = textTransform.batch2Train(voc.word_to_index, voc.vocab_pairs[:5])\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
    "\n",
    "print(\"input_variable:\", input_variable)\n",
    "print(\"lengths:\", lengths)\n",
    "print(\"target_variable:\", target_variable)\n",
    "print(\"mask:\", mask)\n",
    "print(\"max_target_len:\", max_target_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "            \n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, \n",
    "                          dropout = (0 if n_layers == 1 else dropout),\n",
    "                         bidirectional = True)\n",
    "        \n",
    "    def forward(self, input_seq, input_length, hidden = None):\n",
    "        embedded = self.embedding(input_seq)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_length)\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
    "        return outputs, hidden      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method == \"general\":\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "        elif self.method == \"concat\":\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = torch.FloatTensor(hidden_size)\n",
    "        \n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "    def general_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(endoder_output)\n",
    "        return torch.sum(energy * hidden, dim=2)\n",
    "\n",
    "    def concat_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), \n",
    "                                      encoder_output), 2)).tanh()\n",
    "        return torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        if self.method == \"general\":\n",
    "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "        elif self.method == \"concat\":\n",
    "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
    "        elif self.method == \"dot\":\n",
    "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "\n",
    "        attn_energies = attn_energies.t()\n",
    "\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, embedding, hidden_size, output_size,  n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        self.attn_model = attn_model\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout = (0 if n_layers==1 else dropout))\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "        \n",
    "    \n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        \n",
    "        rnn_output, decoder_hidden = self.gru(embedded, last_hidden)\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0,1))\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        \n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        \n",
    "        return output, decoder_hidden\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 100\n",
    "n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 5\n",
    "num_words = len(voc.word_to_index)\n",
    "embedding = nn.Embedding(num_words, 100)\n",
    "\n",
    "encoder = EncoderRNN(100, embedding)\n",
    "\n",
    "outputs, hidden = encoder(input_variable, lengths)\n",
    "\n",
    "decoder_hidden = hidden[::2]\n",
    "decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
    "decoder_input = embedding(decoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 100])"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = Attn('concat', 100)\n",
    "gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
    "rnn_output, _hidden = gru(decoder_input, decoder_hidden)\n",
    "scores = attn(rnn_output, outputs)\n",
    "torch.cat((rnn_output.expand(1, 5,100), outputs[:1,:,:]), 2).shape\n",
    "#rnn_output.squeeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = nn.Linear(hidden_size, num_words)\n",
    "concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "context = scores.bmm(outputs.transpose(0,1))\n",
    "rnn_output = rnn_output.squeeze(0)\n",
    "context = context.squeeze(1)\n",
    "concat_output = torch.cat((rnn_output, context), 1)\n",
    "concat_output = torch.tanh(concat(concat_output))\n",
    "\n",
    "output = out(concat_output)\n",
    "output = F.softmax(output, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 28204])"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [4]])"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([[1,2],[3,4]])\n",
    "torch.gather(t, 1, torch.tensor([[0],[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskNLLLoss(inp, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "    crossEntropy = -torch.log(torch.gather(inp, 1 , target.view(-1,1)).squeeze(1))\n",
    "    loss = crossEntropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n",
    "          encoder_optimizer, decoder_optimizer, batch_size, clip):\n",
    "    \n",
    "    loss = 0\n",
    "    n_total = 0\n",
    "    losses_time_step = []\n",
    "    \n",
    "    input_variable = input_variable.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "    \n",
    "    \n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    encoder_outputs, encoder_hidden, _ = encoder(input_variable, lengths)\n",
    "    \n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "    \n",
    "    decoder_hidden = encoder_hidden[::2]\n",
    "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
    "    \n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_input = target_variable[t].view(1,-1)\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            losses_time_step.append(mask_loss * nTotal)\n",
    "            n_total += nTotal\n",
    "            loss+=mask_loss\n",
    "        \n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            _, top_i = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[top_i[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss_time_step.append(mask_loss * nTotal)\n",
    "            n_total += nTotal\n",
    "            loss+=mask_loss\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    _ = nn.utils.clip_grad_norm(encoder.parameters(), clip)\n",
    "    _ = nn.utils.clip_grad_norm(decoder.parameters(), clip)\n",
    "    \n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return sum(losses_time_step)/n_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterTrain(model_name, voc, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, \n",
    "              print_every, save_every, clip, corpus_name, loadFilename):\n",
    "    \n",
    "    training_batches = [textTransform.batch2Train(voc.word_to_index, [random.choice(voc.vocab_pairs) for _ in range(batch_size)])\n",
    "                        for _ in range(n_iteration)]\n",
    "    \n",
    "    print(\"Initializing ...\")\n",
    "    start_iter = 1\n",
    "    if loadFilename:\n",
    "        start_iter = checkpoint[\"iteration\"] + 1\n",
    "    \n",
    "    print_loss = 0\n",
    "    for iteration in range(start_iter, n_iteration+1):\n",
    "        training_batch = training_batches[iteration-1]\n",
    "        input_variable, lengths, target_variable, mask, max_target_length = training_batch\n",
    "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
    "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
    "        \n",
    "        print_loss += loss\n",
    "        \n",
    "        if iteration % print_every == 0:\n",
    "            print_avg_loss = print_loss/print_every\n",
    "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\"\n",
    "            .format(iteration, iteration / n_iteration * 100, print_avg_loss))\n",
    "            print_loss = 0\n",
    "            \n",
    "        \n",
    "        if iteration % save_every == 0:\n",
    "            directory = os.path.join(save_dir, model_name, corpus_name, \"{}-{}-{}\" \n",
    "                                     .format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
    "            \n",
    "            if not os.path.exist(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save({\n",
    "                'iteration': iteration,\n",
    "                'en': encoder.state_dict(),\n",
    "                'de': decoder.state_dict(),\n",
    "                'en_opt': encoder_optimizer.state_dict(),\n",
    "                'de_opt': decoder_optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'voc_dict': voc.__dict__,\n",
    "                'embedding': embedding.state_dict()\n",
    "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "            \n",
    "    def forward(self, input_seq, length, max_output_length):\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, length)\n",
    "        decoder_hidden = encoder_hidden[::2]\n",
    "\n",
    "        decoder_input = torch.LongTensor([[SOS_token]], device=device)\n",
    "\n",
    "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=device)\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_score, deoder_input = torch.max(decoder_output, dim=1)\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim = 0)\n",
    "            all_scores = torch.cat((all_scores, decoder_score), dim = 0)\n",
    "\n",
    "            decoder_input = torch.unsqueeze(decoder_input, dim = 0)\n",
    "\n",
    "        return all_tokens, all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_length):\n",
    "    index_batch = [textTransform.index_from_sentence(sentence)]\n",
    "    length = torch.Tensor([len(indexes) for index in index_batch])\n",
    "    \n",
    "    input_batch = torch.LongTensor(index_batch).traspose(0,1)\n",
    "    \n",
    "    input_batch = input_batch.to(device)\n",
    "    length = length.to(device)\n",
    "    \n",
    "    all_tokens, all_scores = searcher(input_batch, length, max_length)\n",
    "    \n",
    "    decoded_words = [voc.index_to_word[token.item()] for token in all_tokens]\n",
    "    return decoded_words\n",
    "\n",
    "def evaluate_input(encoder, decoder, searcher):\n",
    "    input_sentence = ''\n",
    "    while(1):\n",
    "        try:\n",
    "            input_sentence = input('>')\n",
    "            if input_sentence == 'q' or input_sentence == 'quit':\n",
    "                break\n",
    "            input_sentence = voc.normalizeSentence(input_sentence)\n",
    "            output_words = searcher(encoder, decoder, input_sentence, max_length)\n",
    "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "            \n",
    "            print('BOT: {}'.format(' '.join(output_words)))\n",
    "        \n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building encoder and decoder...\n"
     ]
    }
   ],
   "source": [
    "model_name = 'cb_model'\n",
    "attn_model = 'dot'\n",
    "hidden_size = 500\n",
    "encoder_n_layer = 2\n",
    "decoder_n_layer = 2\n",
    "dropout = 1\n",
    "batch_size = 64\n",
    "\n",
    "loadFileName = None\n",
    "checkpoint_iter = 4000\n",
    "\n",
    "if loadFileName:\n",
    "    checkpoint = torch.load(loadFileName)\n",
    "    \n",
    "    encoder_sd = checkpoint[\"en\"]\n",
    "    decoder_sd = checkpoint[\"de\"]\n",
    "    \n",
    "    encoder_optimizer_sd = checkpoint[\"en_opt\"]\n",
    "    decoder_optimizer_sd = checkpoint[\"de_opt\"]\n",
    "    \n",
    "    embedding_sd = checkpoint[\"embedding\"]\n",
    "    voc.__dict__ = checkpoint['voc_dict']\n",
    "    \n",
    "\n",
    "print(\"Building encoder and decoder...\")\n",
    "\n",
    "embedding = nn.Embedding(num_words, hidden_size)\n",
    "encoder = EncoderRNN(hidden_size, embedding, n_layers, dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, \n",
    "                              decoder_n_layer, num_words, dropout)\n",
    "\n",
    "if loadFileName:\n",
    "    embedding = embedding.load_state_dict(embedding_sd)\n",
    "    encoder = encoder.load_state_dict(encoder_sd)\n",
    "    decoder = decoder.load_state_dict(decoder_sd)\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "print(\"Model is ready for training!!!\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building optimizers\n"
     ]
    }
   ],
   "source": [
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iterations = 4000\n",
    "print_every = 1\n",
    "save_every = 500\n",
    "save_dir = 'models'\n",
    "corpus_name = \"cornell_movie_dialogs_corpus\"\n",
    "\n",
    "\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "print(\"Building optimizers\")\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "\n",
    "if loadFileName:\n",
    "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
    "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
    "\n",
    "\n",
    "for state in encoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n",
    "\n",
    "for state in decoder_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding, encoder_optimizer\n",
    "#       ,decoder_optimizer, 5, clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:46: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:47: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1; Percent complete: 0.0%; Average loss: 10.2326\n",
      "Iteration: 2; Percent complete: 0.1%; Average loss: 10.2289\n",
      "Iteration: 3; Percent complete: 0.1%; Average loss: 10.2277\n",
      "Iteration: 4; Percent complete: 0.1%; Average loss: 10.2222\n",
      "Iteration: 5; Percent complete: 0.1%; Average loss: 10.2183\n",
      "Iteration: 6; Percent complete: 0.1%; Average loss: 10.2137\n",
      "Iteration: 7; Percent complete: 0.2%; Average loss: 10.2096\n",
      "Iteration: 8; Percent complete: 0.2%; Average loss: 10.1951\n",
      "Iteration: 9; Percent complete: 0.2%; Average loss: 10.1967\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-463-9ac02cb83414>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m iterTrain(model_name, voc, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding,\n\u001b[1;32m      2\u001b[0m          \u001b[0mencoder_n_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_n_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m          save_every, clip, corpus_name, loadFileName)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-462-2d40b70fded7>\u001b[0m in \u001b[0;36miterTrain\u001b[0;34m(model_name, voc, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0minput_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_target_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n\u001b[0;32m---> 17\u001b[0;31m                      decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprint_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-461-5921456fbe40>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mmask_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iterTrain(model_name, voc, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding,\n",
    "         encoder_n_layer, decoder_n_layer, save_dir, n_iterations, batch_size, print_every,\n",
    "         save_every, clip, corpus_name, loadFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140800"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(voc.vocab_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
